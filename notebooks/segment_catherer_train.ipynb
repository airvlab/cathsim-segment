{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab946405ccfffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using Intel Arc GPU (like I am), run this cell.\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87329ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = os.path.join(\"../\", \"guide3d/data/guide3d\")\n",
    "ANNOTATION_FILE_PATH = os.path.join(DATA_ROOT_PATH, \"annotations.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ANNOTATION_FILE_PATH, 'r') as f:\n",
    "    xml_data = f.read()\n",
    "\n",
    "xml_parsed = BeautifulSoup(xml_data, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, read some arbitrary image.\n",
    "# some_img = os.path.join(DATA_ROOT_PATH, xml_parsed.camera[\"image\"])\n",
    "some_img = os.path.join(DATA_ROOT_PATH, \"1-bca-straight-1-2\", \"241.png\")\n",
    "some_img = Image.open(some_img)\n",
    "\n",
    "some_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.adaptiveThreshold(np.array(some_img), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    " cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "plt.imshow(edges, cmap = \"gray\", vmin = 0, vmax = 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(some_img)[592-40:592+40, 192-40:192+40], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d24193",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(some_img)[301-80:301+80, 508-80:508+80], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb0cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array([])\n",
    "\n",
    "\n",
    "camera_xml_nodes = xml_parsed.findAll(\"camera\")\n",
    "reconstruction_xml_nodes = xml_parsed.findAll(\"reconstruction\")\n",
    "\n",
    "for each_camera_node in camera_xml_nodes:\n",
    "    mask = np.zeros((1024, 1024))\n",
    "    images = np.append(images, each_camera_node[\"image\"])\n",
    "\n",
    "print(images[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((1024, 1024))\n",
    "x = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "for i in range(len(each_camera_node[\"points\"].split(';'))):\n",
    "    j, k = each_camera_node[\"points\"].split(';')[i].split(',')\n",
    "    print(k, j)\n",
    "\n",
    "    x = np.append(x, int(j))\n",
    "    y = np.append(y, int(k))\n",
    "\n",
    "    mask[int(k)][int(j)] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(mask), interpolation = \"nearest\", aspect = \"auto\", cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(some_img), interpolation = \"nearest\", aspect = \"auto\")\n",
    "# plt.scatter(x, y, color = \"red\")\n",
    "\n",
    "# Morph points into a polyline.\n",
    "\n",
    "x = np.int32(x)\n",
    "y = np.int32(y)\n",
    "\n",
    "mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)\n",
    "mask_points = mask_points.reshape((-1, 1, 2))\n",
    "\n",
    "closed_img = cv2.polylines(np.array(some_img), [mask_points], isClosed = False, color = (0, 0, 255), thickness = 2)\n",
    "\n",
    "plt.imshow(closed_img, aspect = \"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49360782",
   "metadata": {},
   "source": [
    "### Image Training for Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba63485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_points = np.subtract(closed_img, np.array(some_img))\n",
    "plt.imshow(mask_points, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59387ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetImagesList():\n",
    "    images = np.array([])\n",
    "\n",
    "    camera_xml_nodes = xml_parsed.findAll(\"camera\")\n",
    "\n",
    "    for each_camera_node in tqdm(camera_xml_nodes):\n",
    "        curr_img = each_camera_node[\"image\"]\n",
    "        images = np.append(images, curr_img)\n",
    "\n",
    "    return images\n",
    "\n",
    "def GetPoints(image_name):\n",
    "    for each_camera_node in xml_parsed.findAll(\"camera\"):\n",
    "        if (each_camera_node[\"image\"] == image_name):\n",
    "            return each_camera_node[\"points\"]\n",
    "\n",
    "        # x = np.array([])\n",
    "        # y = np.array([])\n",
    "        \n",
    "        # for i in range(len(each_camera_node[\"points\"].split(';'))):\n",
    "        #     j, k = each_camera_node[\"points\"].split(';')[i].split(',')\n",
    "\n",
    "        #     x = np.append(x, np.int32(j))\n",
    "        #     y = np.append(y, np.int32(k))\n",
    "\n",
    "    #     mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)\n",
    "    #     mask_points = mask_points.reshape((-1, 1, 2))\n",
    "    #     mask_points = np.int32(mask_points)\n",
    "\n",
    "    #     # For the purpose of forming a polyline, we need to get the image data.\n",
    "\n",
    "    #     tmp_img = os.path.join(DATA_ROOT_PATH, curr_img)\n",
    "    #     tmp_img = np.array(Image.open(tmp_img), np.int32)\n",
    "\n",
    "    #     closed_img = cv2.polylines(tmp_img, [mask_points], isClosed = False, color = (0, 0, 255), thickness = 2)\n",
    "    #     segm = np.subtract(closed_img, tmp_img)\n",
    "\n",
    "    #     print(segm.shape)\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd54a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = GetImagesList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641557af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(os.path.join(DATA_ROOT_PATH, a[1000]))\n",
    "\n",
    "plt.imshow(np.array(test_img))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmDataLoader(Dataset):\n",
    "    def __init__(self, transform = None, target_transform = None):\n",
    "        self.images = GetImagesList()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # We return both the image matrix and segmentation matrix.\n",
    "        points = GetPoints(self.images[idx])\n",
    "\n",
    "        x = np.array([])\n",
    "        y = np.array([])\n",
    "        \n",
    "        for i in range(len(points.split(';'))):\n",
    "            j, k = points.split(';')[i].split(',')\n",
    "\n",
    "            x = np.append(x, np.int32(j))\n",
    "            y = np.append(y, np.int32(k))\n",
    "\n",
    "        mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)\n",
    "        mask_points = mask_points.reshape((-1, 1, 2))\n",
    "        mask_points = np.int32(mask_points)\n",
    "\n",
    "        # For the purpose of forming a polyline, we need to get the image data.\n",
    "\n",
    "        _tmp_img = os.path.join(DATA_ROOT_PATH, self.images[idx])\n",
    "        _tmp_img = Image.open(_tmp_img)\n",
    "\n",
    "        _closed_img = cv2.polylines(np.array(_tmp_img), [mask_points], isClosed = False, color = (255, 0, 0), thickness = 2)\n",
    "        _segm = np.subtract(_tmp_img, _closed_img)\n",
    "\n",
    "        _tmp_img = np.array(_tmp_img)\n",
    "        _segm = np.where(_segm != 0, 1, 0)\n",
    "\n",
    "        # Finally, ensure both _tmp_img and _segm are tensors of current dtype that can carry gradient information.\n",
    "        _tmp_img = torch.tensor(_tmp_img, dtype = torch.float32)\n",
    "        _segm = torch.tensor(_segm, dtype = torch.long)\n",
    "        \n",
    "        return self.images[idx], _tmp_img, _segm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61198f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = SegmDataLoader()\n",
    "\n",
    "for i in test_dl:\n",
    "    _, tmp_img, segm = i\n",
    "\n",
    "    plt.imshow(segm, cmap = \"gray\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(1024*1024, 256)\n",
    "        self.linear_2 = nn.Linear(256, 256)\n",
    "        self.linear_3 = nn.Linear(256, 1024*1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lg_softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_3(x)\n",
    "        x = self.lg_softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "model = Network().to(\"xpu\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "test_dl = SegmDataLoader()\n",
    "loss_f = nn.NLLLoss().to(\"xpu\")\n",
    "TOTAL_EPOCHS = 3\n",
    "\n",
    "losses = np.array([])\n",
    "optimiser = SGD(model.parameters(), lr=0.00001)\n",
    "\n",
    "print(model.parameters())\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for epoch in range(TOTAL_EPOCHS):\n",
    "        optimiser.zero_grad()\n",
    "        curr_losses = np.array([])\n",
    "\n",
    "        i = 0\n",
    "        for single_data in tqdm(test_dl):\n",
    "            _, tmp_img, segm = single_data\n",
    "            tmp_img = tmp_img.reshape(-1).to(\"xpu\")\n",
    "            \n",
    "            Y_preds = model(tmp_img)\n",
    "\n",
    "            segm = segm.reshape(-1).to(\"xpu\")\n",
    "\n",
    "            loss = loss_f(Y_preds, segm)\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            curr_losses = np.append(curr_losses, loss.cpu().detach())\n",
    "\n",
    "            losses = np.append(losses, np.mean(curr_losses))\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                print(\"EPOCH MIN MAX\", epoch, np.min(losses), np.max(losses))\n",
    "\n",
    "\n",
    "            i += 1\n",
    "        if epoch == 2:\n",
    "            torch.save(model, \"trained_model_3_epochs\")\n",
    "            torch.save(model.state_dict(), \"trained_model_3_epochs_statedict\")\n",
    "\n",
    "            break\n",
    "\n",
    "    del curr_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cbf10",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fc35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"trained_model_3_epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c799d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
