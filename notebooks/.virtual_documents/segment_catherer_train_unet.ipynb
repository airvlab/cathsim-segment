import os
import torch
import cv2

import matplotlib.pyplot as plt
import numpy as np

from bs4 import BeautifulSoup
from PIL import Image


# If using Intel Arc GPU (like I am), run this cell.
import intel_extension_for_pytorch as ipex


DATA_ROOT_PATH = os.path.join("../", "guide3d/data/guide3d")
ANNOTATION_FILE_PATH = os.path.join(DATA_ROOT_PATH, "annotations.xml")


with open(ANNOTATION_FILE_PATH, 'r') as f:
    xml_data = f.read()

xml_parsed = BeautifulSoup(xml_data, "lxml")


# For testing, read some arbitrary image.
# some_img = os.path.join(DATA_ROOT_PATH, xml_parsed.camera["image"])
some_img = os.path.join(DATA_ROOT_PATH, "1-bca-straight-1-2", "241.png")
some_img = Image.open(some_img)

some_img.size


edges = cv2.adaptiveThreshold(np.array(some_img), 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\
 cv2.THRESH_BINARY, 11, 2)

plt.imshow(edges, cmap = "gray", vmin = 0, vmax = 255)



plt.imshow(np.array(some_img)[592-40:592+40, 192-40:192+40], cmap = "gray")


plt.imshow(np.array(some_img)[301-80:301+80, 508-80:508+80], cmap = "gray")


images = np.array([])


camera_xml_nodes = xml_parsed.findAll("camera")
reconstruction_xml_nodes = xml_parsed.findAll("reconstruction")

for each_camera_node in camera_xml_nodes:
    mask = np.zeros((1024, 1024))
    images = np.append(images, each_camera_node["image"])

print(images[2])



mask = np.zeros((1024, 1024))
x = np.array([])
y = np.array([])

for i in range(len(each_camera_node["points"].split(';'))):
    j, k = each_camera_node["points"].split(';')[i].split(',')
    print(k, j)

    x = np.append(x, int(j))
    y = np.append(y, int(k))

    mask[int(k)][int(j)] = 1
    


plt.imshow(np.array(mask), interpolation = "nearest", aspect = "auto", cmap = "gray")


plt.imshow(np.array(some_img), interpolation = "nearest", aspect = "auto")
# plt.scatter(x, y, color = "red")

# Morph points into a polyline.

x = np.int32(x)
y = np.int32(y)

mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)
mask_points = mask_points.reshape((-1, 1, 2))

closed_img = cv2.polylines(np.array(some_img), [mask_points], isClosed = False, color = (0, 0, 255), thickness = 2)

plt.imshow(closed_img, aspect = "auto")





import torch
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader

from torch import nn
from torch.optim import SGD

from torchsummary import summary

from tqdm import tqdm

import warnings
warnings.filterwarnings("ignore")


mask_points = np.subtract(closed_img, np.array(some_img))
plt.imshow(mask_points, cmap = "gray")


def GetImagesList():
    images = np.array([])

    camera_xml_nodes = xml_parsed.findAll("camera")

    for each_camera_node in tqdm(camera_xml_nodes):
        curr_img = each_camera_node["image"]
        images = np.append(images, curr_img)

    return images

def GetPoints(image_name):
    for each_camera_node in xml_parsed.findAll("camera"):
        if (each_camera_node["image"] == image_name):
            return each_camera_node["points"]

        # x = np.array([])
        # y = np.array([])
        
        # for i in range(len(each_camera_node["points"].split(';'))):
        #     j, k = each_camera_node["points"].split(';')[i].split(',')

        #     x = np.append(x, np.int32(j))
        #     y = np.append(y, np.int32(k))

    #     mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)
    #     mask_points = mask_points.reshape((-1, 1, 2))
    #     mask_points = np.int32(mask_points)

    #     # For the purpose of forming a polyline, we need to get the image data.

    #     tmp_img = os.path.join(DATA_ROOT_PATH, curr_img)
    #     tmp_img = np.array(Image.open(tmp_img), np.int32)

    #     closed_img = cv2.polylines(tmp_img, [mask_points], isClosed = False, color = (0, 0, 255), thickness = 2)
    #     segm = np.subtract(closed_img, tmp_img)

    #     print(segm.shape)
    #     break


a = GetImagesList()


test_img = Image.open(os.path.join(DATA_ROOT_PATH, a[1000]))

plt.imshow(np.array(test_img))




class SegmDataLoader(Dataset):
    def __init__(self, transform = None, target_transform = None):
        self.images = GetImagesList()
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # We return both the image matrix and segmentation matrix.
        points = GetPoints(self.images[idx])

        x = np.array([])
        y = np.array([])
        
        for i in range(len(points.split(';'))):
            j, k = points.split(';')[i].split(',')

            x = np.append(x, np.int32(j))
            y = np.append(y, np.int32(k))

        mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)
        mask_points = mask_points.reshape((-1, 1, 2))
        mask_points = np.int32(mask_points)

        # For the purpose of forming a polyline, we need to get the image data.

        _tmp_img = os.path.join(DATA_ROOT_PATH, self.images[idx])
        _tmp_img = Image.open(_tmp_img)

        _closed_img = cv2.polylines(np.array(_tmp_img), [mask_points], isClosed = False, color = (255, 0, 0), thickness = 2)
        _segm = np.subtract(_tmp_img, _closed_img)

        _tmp_img = np.array(_tmp_img)
        _segm = np.where(_segm != 0, 1, 0)

        # Finally, ensure both _tmp_img and _segm are tensors of current dtype that can carry gradient information.
        _tmp_img = torch.tensor(_tmp_img, dtype = torch.float32)
        _segm = torch.tensor(_segm, dtype = torch.long)
        
        return self.images[idx], _tmp_img, _segm


test_dl = SegmDataLoader()

for i in test_dl:
    _, tmp_img, segm = i

    plt.imshow(segm, cmap = "gray")

    break


class Network(nn.Module):
    def __init__(self):
        super().__init__()

        self.linear_1 = nn.Linear(1024*1024, 256)
        self.linear_2 = nn.Linear(256, 256)
        self.linear_3 = nn.Linear(256, 1024*1024)
        self.relu = nn.ReLU()
        self.lg_softmax = nn.LogSoftmax()

    def forward(self, x):
        x = self.linear_1(x)
        x = self.relu(x)
        x = self.linear_2(x)
        x = self.relu(x)
        x = self.linear_3(x)
        x = self.lg_softmax(x)

        return x


# del model
# import gc
# gc.collect()

model = Network().to("cuda")
print(model)


from torch.autograd import Variable

train_ds = SegmDataLoader()
train_dl = DataLoader(train_ds, batch_size = 124, shuffle = True, num_workers = 2)
loss_f = nn.NLLLoss().to("xpu")
TOTAL_EPOCHS = 3

losses = np.array([])
optimiser = SGD(model.parameters(), lr=0.00001)

print(model.parameters())

with torch.set_grad_enabled(True):
    torch.cuda.empty_cache()

    for epoch in range(TOTAL_EPOCHS):
        optimiser.zero_grad()
        curr_losses = np.array([])

        i = 0
        for single_data in tqdm(test_dl):
            _, tmp_img, segm = single_data
            tmp_img = tmp_img.reshape(-1).to("cuda")
            
            Y_preds = model(tmp_img)

            segm = segm.reshape(-1).to("cuda")

            loss = loss_f(Y_preds, segm)

            loss.backward()
            optimiser.step()

            curr_losses = np.append(curr_losses, loss.cpu().detach())

            losses = np.append(losses, np.mean(curr_losses))

            if i % 500 == 0:
                print("EPOCH MIN MAX", epoch, np.min(losses), np.max(losses))


            i += 1
        if epoch == 2:
            torch.save(model, "trained_model_3_epochs")
            torch.save(model.state_dict(), "trained_model_3_epochs_statedict")

            break

    del curr_losses





model = torch.load("trained_model_3_epochs")





class SegmDataLoader(Dataset):
    def __init__(self, transform = None, target_transform = None):
        self.images = GetImagesList()[0:4000]
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # We return both the image matrix and segmentation matrix.
        points = GetPoints(self.images[idx])

        x = np.array([])
        y = np.array([])
        
        for i in range(len(points.split(';'))):
            j, k = points.split(';')[i].split(',')

            x = np.append(x, np.int32(j))
            y = np.append(y, np.int32(k))

        mask_points = np.concatenate([x[:,None], y[:,None]], axis = 1)
        mask_points = mask_points.reshape((-1, 1, 2))
        mask_points = np.int32(mask_points)

        # For the purpose of forming a polyline, we need to get the image data.

        _tmp_img = os.path.join(DATA_ROOT_PATH, self.images[idx])
        _tmp_img = Image.open(_tmp_img)

        _closed_img = cv2.polylines(np.array(_tmp_img), [mask_points], isClosed = False, color = (255, 0, 0), thickness = 2)
        _segm = np.subtract(_tmp_img, _closed_img)

        _tmp_img = np.array(_tmp_img)
        _segm = np.where(_segm != 0, 1, 0)

        _tmp_img = (_tmp_img - _tmp_img.min())/(_tmp_img.max() - _tmp_img.min())


        _tmp_img = np.transpose(np.stack((_tmp_img,) * 3, axis = -1)).reshape(1, 3, 1024, 1024)
        _segm = np.transpose(_segm)
        # _segm = np.transpose(np.stack((_segm,) * 3, axis = -1)).reshape(1, 3, 1024, 1024)
        # Finally, ensure both _tmp_img and _segm are tensors of current dtype that can carry gradient information.
        _tmp_img = torch.tensor(_tmp_img, dtype = torch.float32)
        _segm = torch.tensor(_segm, dtype = torch.long)
        
        return self.images[idx], _tmp_img, _segm


import torch
import torch.nn as nn
from torchvision import models

from torch.utils.tensorboard import SummaryWriter

def convrelu(in_channels, out_channels, kernel, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),
        nn.ReLU(inplace=True),
    )

class UNet(nn.Module):
    def __init__(self, n_class):
        super().__init__()
        
        self.base_model = models.resnet18()
        self.base_model.load_state_dict(torch.load("./resnet18.pth"))
        self.base_layers = list(self.base_model.children())

        self.layer0 = nn.Sequential(*self.base_layers[:3])
        self.layer0_1x1 = convrelu(64, 64, 1, 0)
        self.layer1 = nn.Sequential(*self.base_layers[3:5])
        self.layer1_1x1 = convrelu(64, 64, 1, 0)
        self.layer2 = self.base_layers[5]
        self.layer2_1x1 = convrelu(128, 128, 1, 0)
        self.layer3 = self.base_layers[6]
        self.layer3_1x1 = convrelu(256, 256, 1, 0)
        self.layer4 = self.base_layers[7]
        self.layer4_1x1 = convrelu(512, 512, 1, 0)

        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)
        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)
        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)
        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)

        self.conv_original_size0 = convrelu(3, 64, 3, 1)
        self.conv_original_size1 = convrelu(64, 64, 3, 1)
        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)

        self.conv_last = nn.Conv2d(64, n_class, 1)

    def forward(self, input):
        x_original = self.conv_original_size0(input)
        x_original = self.conv_original_size1(x_original)

        layer0 = self.layer0(input)
        layer1 = self.layer1(layer0)
        layer2 = self.layer2(layer1)
        layer3 = self.layer3(layer2)
        layer4 = self.layer4(layer3)

        layer4 = self.layer4_1x1(layer4)
        x = self.upsample(layer4)
        layer3 = self.layer3_1x1(layer3)
        x = torch.cat([x, layer3], dim=1)
        x = self.conv_up3(x)

        x = self.upsample(x)
        layer2 = self.layer2_1x1(layer2)
        x = torch.cat([x, layer2], dim=1)
        x = self.conv_up2(x)

        x = self.upsample(x)
        layer1 = self.layer1_1x1(layer1)
        x = torch.cat([x, layer1], dim=1)
        x = self.conv_up1(x)

        x = self.upsample(x)
        layer0 = self.layer0_1x1(layer0)
        x = torch.cat([x, layer0], dim=1)
        x = self.conv_up0(x)

        x = self.upsample(x)
        x = torch.cat([x, x_original], dim=1)
        x = self.conv_original_size2(x)

        out = self.conv_last(x)

        return out


# import gc
# del model
# gc.collect()
# torch.cuda.empty_cache()

device = "cuda"
model = UNet(n_class=2)
model = model.to(device)

print(summary(model, input_size = (3, 1024, 1024)), device)



train_ds = SegmDataLoader()
loss_f = nn.CrossEntropyLoss().to("cuda")
TOTAL_EPOCHS = 50

losses = np.array([])
optimiser = SGD(model.parameters(), lr=0.0001)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimiser,T_max = 30)

tensorboard_writer = SummaryWriter()

print(model.parameters())

with torch.set_grad_enabled(True):
    torch.cuda.empty_cache()

    for epoch in range(TOTAL_EPOCHS):
        optimiser.zero_grad()
        curr_losses = np.array([])

        i = 0
        for single_data in tqdm(train_ds):
            _, tmp_img, segm = single_data
            tmp_img = tmp_img.to("cuda")
            
            Y_preds = model(tmp_img)

            # segm = segm.reshape(-1)
            segm = segm.reshape(1, 1024, 1024).to("cuda")

            # print(tmp_img.shape, segm.shape)

            loss = loss_f(Y_preds, segm)

            loss.backward()
            optimiser.step()
            scheduler.step()

            curr_losses = np.append(curr_losses, loss.cpu().detach())

            losses = np.append(losses, np.mean(curr_losses))

            if i % 500 == 0:
                tensorboard_writer.add_scalar("Loss/train", loss.cpu().detach(), epoch)
                print("EPOCH MIN MAX", epoch, np.min(losses), np.max(losses))


            i += 1
            
        torch.save(model, f"trained_model_{epoch}_epochs.pth")
        torch.save(model.state_dict(), f"trained_model_{epoch}_epochs_statedict.pth")

    del curr_losses


plt.imshow(tmp_img.cpu().detach().numpy()[0, 0, :, :], cmap = "gray")

print(tmp_img.cpu().detach().numpy()[0, 0, :, :].shape)


plt.imshow(segm.cpu().detach().numpy()[:, :], cmap = "gray")
print(segm.shape)


plt.imshow(torch.sigmoid(model(tmp_img)).cpu().detach().numpy()[0, 0, :, :], cmap = "gray")


size = 1024, 1024
m = UNet(n_class = 2)
m = torch.load("trained_model_1_epochs.pth")

test_img = Image.open("./TestImg.png").convert('L')

test_img = test_img.resize(size)
test_img = np.array(test_img, np.float32)

test_img = np.transpose(np.stack((test_img,) * 3, axis = -1)).reshape(1, 3, 1024, 1024)
plt.imshow(test_img[0, 0, :, :], cmap = "gray")

test_img = (test_img - test_img.min())/(test_img.max() - test_img.min())

print(np.min(test_img), np.max(test_img))

test_img = torch.tensor(test_img).reshape(1, 3, 1024, 1024).to("cuda")

Y_preds = m(test_img).cpu().detach()
print(Y_preds.shape)


Y = torch.sigmoid(Y_preds).cpu().detach().numpy()

plt.imshow(Y[0, 0, :, :], cmap = "gray")


Y[0, 0, :, :]






